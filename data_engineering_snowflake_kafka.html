<!DOCTYPE HTML>
<html>
	<head>
		<title>Paul Hankins</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-dark.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/toolbar/prism-toolbar.min.css" integrity="sha512-Dqf5696xtofgH089BgZJo2lSWTvev4GFo+gA2o4GullFY65rzQVQLQVlzLvYwTo0Bb2Gpb6IqwxYWtoMonfdhQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.css" integrity="sha512-cbQXwDFK7lj2Fqfkuxbo5iD1dSbLlJGXGpfTDqbggqjHJeyzx88I3rfwjS38WJag/ihH7lzuGlGHpDBymLirZQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo">Home</a>
				</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">About me</a></li>
                            <li  class=""><a href="2024-engineer-projects.html">Engineer Projects 2024</a></li>
							<li  class="active"><a href="engineer-projects-4.html">Engineer Projects 2022</a></li>
                            <li><a href="analytics-projects-1.html">Analytics Projects 2022</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/paul-hankins-075119204/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
							<li><a href="https://github.com/Piblo99" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
                                <header class="major">
                                    <h1>Snowflake Data Engineering<br /></h1>
                                    <p>The objective of this project is to explore concepts of data engineering principles
                                        with snowlake, apache kafka, apache airflow, and AWS. For every section in this page,
                                        I have worked hands on with the features described to get a better understanding of them
                                        and have provided many examples with links to my github files.
                                    </p>
                                </header>
                                <h2 style="text-align:center;">Consumption and Billing</h2>
                                <p>
                                    Snowflake can be costly. Thankfully Snowflake consumption can be tracked with some SQL Queries!
                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%202%20Consumption%20%26%20Billing/consumption_tracking.sql">Click here for an example</a> 
                                </p>

                                <h2 style="text-align:center;">Snowflake Objects</h2>
                                <p>
                                    There are several different types of tables in snowflake.
                                </p>
                                <ul>
                                    <li>Permanent Tables are the default type of tables.</li>
                                    <li>Temporary Tables are used for results of ad hoc queries that wouldn't be expected to be needed for a long time. This type of table is deleted when a session ends.</li>
                                    <li>Transient Tables are similar to Permanent tables. This type of table exists until they are explicitly dropped. Used to persist transitory data that is needed beyond a session.</li>
                                </ul>
                                <p>
                                    Temporary and transient tables do not have fale-safe and time-travel features enabled.
                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%203%20Different%20Type%20of%20Tables/table_types.sql"> Click here to see an exploration of table types</a>
                                </p>

                                <br>

                                <p>
                                    As for views in snowflake there are Materialized views and non-materialized views.
                                    <br>
                                    The characteristics of materialized views are as follows:
                                </p>
                                <ul>
                                    <li>A materialized view is a per-computed data set derived from a query specification (The select in the view definition) and stored for later use.</li>
                                    <li>Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view</li>
                                    <li>Materialized views can speed up expensive aggregation, projection, and selection operations, especially those that run frequently and those that run on large datasets.</li>
                                </ul>
                                <p>
                                    Views and snowflake can also be defined as secure or insecure.
                                    <br>
                                    The characteristics view security are as follows:
                                </p>
                                <ul>
                                    <li>Views should be defined as secure when they are specifically designated for data privacy.</li>
                                    <li>With secure views, the view definition and details are only visible to authorized users</li>
                                    <li>Both materialized and non-materialized views can be defined as secure</li>
                                    <li>A Secure View has improved data privacy and data sharing.</li>
                                    <li>Both views and materialized views can be clustered for better performance.</li>
                                </ul>
                                <p>
                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%203%20Different%20Type%20of%20Tables/views-materialized_views.sql">Click here for an example of the creation and securing of views.</a>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%203%20Different%20Type%20of%20Tables/create-role-to-test-security.sql">Click here for an example of using roles to grant permissions to use secure views.</a>
                                </p>
                                <h2 style="text-align:center;">Partitioning, Clustering, and Performance Optimization</h2>
                                <p>
                                    In snowflake the data inside the table automatically gets divided into smaller chunks of data 
                                    in the range of 50MB to 500MB each called micro-partitions. These micro partitions do not need to be defined 
                                    explicitly & can overlap in the range of values.
                                    The metadata for each of these partitions is stored in the cloud services layer.
                                    <br>
                                    The metadata contains:
                                    <ul>
                                        <li>
                                            Range of values for each of the columns in micro-partitions
                                        </li>
                                        <li>
                                            Number of unique values
                                        </li>
                                        <li>
                                            Additional properties required for efficient query processing and data scanning
                                        </li>       
                                    </ul>

                                    Clustering is characterized by:
                                    <ul>
                                        <li>
                                            Clustering is a process to optimize data retrieval.
                                        </li>
                                        <li>
                                            Clustering is performed on micro-partitions to ensure similar data reside in the same micro-partition which can be fetched in a single query.
                                        </li>
                                        <li>
                                            The Clustering Key is a column or a group of columns that are designated to locate the data in the same micro-partition.
                                        </li>
                                        <li>
                                            Clustering keys play a key role in ensuring the data is sorted/ordered inside the micro-partitions which especially helps while querying large tables.
                                        </li>
                                        <li>
                                            Pruning is a process where snowflake avoids scanning the unnecessary micro-partitions using the clustering keys in the "where clause" thus resulting in better performance.
                                        </li>
                                        <li>
                                            Snowflake automaticalls splits the table into clustered micro-partitions using the natural dimensions such as date columns.
                                        </li>
                                        <li>
                                            These clustered micro-partitions might not be necessarily ideal in the long run. (This is due to the high cardinality associated with these columns.)
                                        </li>
                                    </ul>
                                    <br>
                                    <p>
                                        Considerations for clustering a table in snowflake:
                                    </p>
                                    <ul>
                                        <li>
                                            Table is huge in size. Ideally multiple TBs of data.
                                        </li>
                                        <li>
                                            Majority of the queries are selective (use a where condition on a column or group of columns)
                                        </li>
                                        <li>
                                            Majority of the queries sort the data (use a order by clause)
                                        </li>
                                        <li>
                                            The ratio of queries (reading) and the DML operations should be high.
                                        </li>
                                        <li>
                                            If you want to cluster a large table with a lot of frequent DML operations, consider grouping the DML operations together and running them less frequently.
                                        </li>
                                        <li>
                                            A high number of queries in your entire workload can possibly use the same clustering key.
                                        </li>
                                        <li>
                                            For Example: 95% of the queries in your data ecosystem filter the data and order by using the same set of columns.
                                        </li>
                                    </ul>

                                    <br>
                                    <p> Selecting the clustering keys</p>

                                    <ul>
                                        <li>
                                            A clustering key can be one column or multiple columns. Ideally the max number of columns should be limited to 3.
                                        </li>
                                        <li>
                                            Columns with low and high cardinality are not good candidates for clustering keys and must be avoided.
                                        </li>
                                        <li>
                                            Eg: If you select a timestamp value (high cardinality) or a gender column with extremely low cardinality, this will result in poor performance of your queries and high clustering costs will be incurred.
                                        </li>
                                        <li>
                                            If the clustering key is multiple columns, the order in which the columns are specified in the create table or alter table statement is important. As a rule of thumb, columns must be specified from low to high cardinality.
                                        </li>
                                    </ul>
                                    
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%204%20Micropartitions%20%26%20Clustering/selecting_clustering_keys.sql">Click here to see an example of selecting clustering keys</a>
                                    <br><br>
                                    Snowflake allows you to check the query history and even filter by the types of queries executed. This helps with monitoring for performance checking and security purposes.
                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%204%20Micropartitions%20%26%20Clustering/query_history.sql">Click here to see an example of query history checking</a>
                                    <br><br>
                                    Snowflake allows you to leverage a feature called search optimiation when using point lookup queries.
                                    Point lookup queries are queries in which the where clause is used to select rows for a particular column.
                                    Eg: Select * from table where column = "abc"

                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%204%20Micropartitions%20%26%20Clustering/search_optimization.sql">Click here to see an example Snowflake's search optimization</a>
                                </p>
                                <h2 style="text-align:center;">Data Loading/Ingestion and Extraction</h2>
                                <p>
                                    Snowflake has multiple options for loading data!
                                    Batch Bulk Data Ingestion:
                                    <ul>
                                        <li>
                                            Write/load the data into your staging locations (S3, GCS Buckets)
                                        </li>
                                        <li>
                                            <ul>
                                                Ingest the data into Snowflake in batches at frequent time intervals using:
                                                <li>
                                                    Snowflake Copy Commands scheduled using Snowflake tasks
                                                </li>
                                                <li>
                                                    Trigger Copy commands using python/Glue/Airflow running at specified time intervals
                                                </li>
                                            </ul>
                                        </li>
                                    </ul>
                                    Real-time Data Ingestion:
                                    <ul>
                                        <li>
                                            Write/load the data into your staging location (S3, GCS Buckets) and ingest the data in near-real time using:
                                            <ul>
                                                <li>
                                                    Snowpipe(Continuous data ingestion)
                                                </li>
                                                <li>
                                                    Airflow S3 sensors/triggers
                                                </li>
                                            </ul>
                                        </li>
                                        <li>
                                            Kafka-snowflake Connector real-time data ingestion
                                        </li>
                                    </ul>

                                    I created an AWS connection to Snowflake for learning purposes. the steps are as follows:

                                    <ol>
                                        <li>
                                            Create an IAM Role for snowflake to access data in S3 Buckets
                                        </li>
                                        <li>
                                            Create an S3 Bucket in AWS and upload Files into bucket
                                        </li>
                                        <li>
                                            Create an Integration Object in Snowflake for authentication
                                        </li>
                                        <li>
                                            Create a File Format Object
                                        </li>
                                        <li>
                                            Create a stage object referencing the location from which the data needs to be ingested
                                        </li>
                                        <li>
                                            Load the data into Snowflake Tables
                                        </li>
                                    </ol>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%205%20Data%20Loading%20into%20Snowflake/aws-snowflake-setup.sql">Click here to see the setup of an AWS connection to snowflake</a>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%205%20Data%20Loading%20into%20Snowflake/auto-ingestion-snowpipe.sql">Click here to see an example of using Snowpipe for auto ingestion from AWS S3</a>
                                </p>

                                <h2 style="text-align:center;">Tasks and Query Scheduling</h2>
                                <p>
                                    Snowflake Tasks are similar to a scheduler that can execute SQL queries at a defined frequency or time.
                                    <br><br>
                                    A task can be used to execute:
                                    <ul>
                                        <li>
                                            A single Query
                                        </li>
                                        <li>
                                            Multiple SQL Queries (Tree of tasks)
                                        </li>
                                    </ul>
                                    Compute resources for a task can be:
                                    <br><br>
                                    <ul>
                                        <li>
                                            Serverless (Snowflake managed)
                                        </li>
                                        <li>
                                            User Managed (one of the existing virtual warehouses)
                                        </li>
                                    </ul>

                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%206%20Tasks%20%26%20Query%20Scheduling/create_tasks.sql">Click here to see an example of Task Creation in Snowflake</a>
                                </p>
                                <h2 style="text-align:center;">Streams and Change Data Capture</h2>
                                <p>
                                    CDC is the process of capturing GML operations
                                    (updates, inserts, and deletes) in your tables so that necessary actions can be taken
                                    using the changed data. In Snowflake, the process of CDC is achieved using streams.
                                    A stream (can be thought of as a table), makes the "changed data" available indicating what 
                                    has changed in the source table between two transactional points.

                                    Snowflake stream types:
                                    <ul>
                                        <li>
                                            Standard: Tracks all DML changes to the source table, including inserts, updates, and deletes.
                                        </li>
                                        <li>
                                            Append-only: Tracks row inserts only. Update and delete operations are not recorded.
                                        </li>
                                        <li>
                                            Insert-only: Same as append-only, but for external tables.
                                        </li>
                                    </ul>
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%207%20Streams%20%26%20CDC/standard-delta-streams.sql">Click here to see an example of implementing standard streams</a>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%207%20Streams%20%26%20CDC/append-only-streams.sql">Click here to see an example of implementing append-only streams</a>

                                    A transaction is a sequence of SQL statements that are processed as an atomic unit. All statements in the transaction are either applied (i.e. committed) or undone (i.e. rolled back) together.
                                    Snowflake transactions support commit and rollback.
                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%207%20Streams%20%26%20CDC/streams-in-transactions.sql">Click here to see an example of implementing streams in transactions</a>
                                    <br>
                                    An Alternative to streams is a feature called Change Tracking.
                                    Change tracking is read-only.
                                    Change tracking enables querying change tracking metadata between two points in time without having to create a stream with an explicit transactional offset.
                                    Using the changes clause does not advance the offset.

                                    If you have a table that is not updated frequently and you don't want to consume the changed data at a defined frequency, that would be a more appropriate time to use change tracking.
                                    Streams would be good for scenarios in which you have a table apart of your pipeline that changes frequently and you want to consume the changes to use them for reporting and analytics.
                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%207%20Streams%20%26%20CDC/change-tracking-cdc.sql">Click here to see an example of implementing change tracking</a>
                                </p>

                                <h2 style="text-align:center;">User Defined Functions</h2>
                                <p>
                                    User defined functions let you extend operations that are not
                                    available through system defined functions in Snowflake.
                                    <br>
                                    <br>
                                    UDF's are reusable code which can be written in:
                                    <ul>
                                        <li>
                                            SQL
                                        </li>
                                        <li>
                                            javascript
                                        </li>
                                        <li>
                                            Java
                                        </li>
                                        <li>
                                            Python (External Functions)
                                        </li>
                                    </ul>

                                    SQL UDF's can execute re-usable sql queries whereas Javascript UDF's give you more 
                                    flexibility when it comes to branching or looping

                                    <br>
                                    <br>
                                    UDFs are of 2 types:
                                    <ul>
                                        <li>
                                            Scalar - A scalar function returns one output row.
                                        </li>
                                        <li>
                                            Tabular - Also called a table function, returns zero, one, or multiple rows for each input row.
                                        </li>
                                    </ul>

                                    <br><br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%208%20UDF/sql_udf.sql">Click here for an example of implementing scalar and tabular UDFs in SQL</a>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%208%20UDF/javascript_udf.sql">Click here for an example of implementing UDFs in JavaScript</a>

                                    <br><br>
                                    Snowflake - Query Filter Execution
                                    <ul>
                                        <li>
                                            Load First, filter later:
                                            <ul>
                                                <li>
                                                    Road all rows from the table into memory
                                                </li>
                                                <li>
                                                    Scan the rows in memory, filtering out any rows that do not match the filter 
                                                </li>
                                                <li>
                                                    Select the columns that remain in the memory (select cause)
                                                </li>
                                            </ul>
                                        </li>
                                        
                                        <li>
                                            Early Filtering (Pushdown)
                                            <ul>
                                                <li>
                                                    Do not load/read the records that do not match the filter conditions from the WHERE clause
                                                </li>
                                                <li>
                                                    Scan the remaining rows in memory and select the columns from the select clauses
                                                </li>
                                            </ul>
                                        </li>
                                    </ul>
                                    <br><br>
                                    Pushdown Benefits
                                    <br>
                                    Pushdown improves performance by filtering out unneeded rows as early as possible during query processing.
                                    <br>
                                    Pushdown can also reduce memory consumption. However pushdown can allow confidential data to be exposed indirectly.
                                    <br><br>
                                    SQL UDFS should be defined as secure when they are specifically designated for data privacy.
                                    Secure UDFs should not be used for UDFs that are created for query re-usability or querying convenience.
                                    Snowflakes query optimizer bypasses the optimizations(pushdowns) when it comes to Secure UDFS.
                                    Secure UDFs have a relatively poor query performance compared to REgular UDFs.
                                </p>
                                <h2 style="text-align:center;">External Functions</h2>
                                <p>
                                    An external function calls code that is executed outside of Snowlfake.
                                    The remotely executed code is known as a remote service.
                                    Information sent to a remote service is usually relayed through a proxy service.
                                    An external function is a type of UDF. Unlike other UDFs, an external function does not contain its own code;
                                    instead, the external function calls code that is stored and executed outside Snowflake.

                                    Examples: 
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%209%20External%20Functions/ext-function-integration.sql">Snowflake integration with external functions</a>
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%209%20External%20Functions/lambda_function.py">external lambda function in python</a>
                                    <br>

                                </p>

                                <h2 style="text-align:center;">Realtime Streaming with Kafka and Snowflake</h2>
                                <p>
                                    Kafka connectors can be used to read data from multiple Apache Kafka topics and load the data into a snowflake table.
                                    <br><br>
                                    https://docs.snowflake.com/en/user-guide/kafka-connector-ts.html
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/tree/master/Section%2011%20kafka%20streaming%20snowflake">kafka pipeline producer and configuration files</a>
                                </p>

                                <h2 style="text-align:center;">Data Protection and Governance</h2>
                                <p>
                                    TimeTravel and Failsafe are features in Snowflake to help with data retention.
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%2012%20Data%20Governance%20%26%20Security/time-travel.sql">Here is an example of the time travel feature in Snowflake.</a>

                                    <br><br>
                                    Snowflake allows for Column level data masking policies. This can for example 
                                    be used to keep a reporting intern for seeing certain values that other roles in a company would be allowed to see.
                                    You can see an example of the Masking feature within snowflake here!
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%2012%20Data%20Governance%20%26%20Security/masking_policies.sql">Masking policy example</a>
                                    <br><br>

                                    Snowflake also has features for row level access policies.
                                    This can be very helpful in situations where you would otherwise have to create 
                                    a large amount of secure views.
                                    <br>
                                    <a href="https://github.com/Piblo99/Snowflake---Build---Architect-Data-pipelines-using-AWS/blob/master/Section%2012%20Data%20Governance%20%26%20Security/row-level-policy.sql">Example of Row level policy in Snowflake</a> 
                                </p>
							</section>

					</div>

				<!-- Footer -->
                <footer id="footer">
                    <section class="alt">
                        <h3>Address</h3>
                        <p>BRIDGE CITY, LA</p>
                    </section>

                    <section>
                        <h3>Email</h3>
                        <p>paulhankins99@gmail.com</p>
                    </section>
                    <section>
                        <h3>Social</h3>
                        <ul class="icons alt">
                            <li><a href="https://www.linkedin.com/in/paul-hankins-075119204/" class="icon brands alt fa-linkedin"><span class="label">Linkedin</span></a></li>
                            <li><a href="https://github.com/Piblo99" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
                        </ul>
                    </section>
            </footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
            <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/toolbar/prism-toolbar.min.js" integrity="sha512-st608h+ZqzliahyzEpETxzU0f7z7a9acN6AFvYmHvpFhmcFuKT8a22TT5TpKpjDa3pt3Wv7Z3SdQBCBdDPhyWA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js" integrity="sha512-/kVH1uXuObC0iYgxxCKY41JdWOkKOxorFVmip+YVifKsJ4Au/87EisD1wty7vxN2kAhnWh6Yc8o/dSAXj6Oz7A==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.js" integrity="sha512-BttltKXFyWnGZQcRWj6osIg7lbizJchuAMotOkdLxHxwt/Hyo+cl47bZU0QADg+Qt5DJwni3SbYGXeGMB5cBcw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-python.min.js" integrity="sha512-AKaNmg8COK0zEbjTdMHJAPJ0z6VeNqvRvH4/d5M4sHJbQQUToMBtodq4HaV4fa+WV2UTfoperElm66c9/8cKmQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
	</body>
</html>