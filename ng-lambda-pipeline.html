<!DOCTYPE HTML>
<html>
	<head>
		<title>Paul Hankins</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism-dark.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/toolbar/prism-toolbar.min.css" integrity="sha512-Dqf5696xtofgH089BgZJo2lSWTvev4GFo+gA2o4GullFY65rzQVQLQVlzLvYwTo0Bb2Gpb6IqwxYWtoMonfdhQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/plugins/line-numbers/prism-line-numbers.min.css" integrity="sha512-cbQXwDFK7lj2Fqfkuxbo5iD1dSbLlJGXGpfTDqbggqjHJeyzx88I3rfwjS38WJag/ihH7lzuGlGHpDBymLirZQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo">Home</a>
				</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">About me</a></li>
							<li  class="active"><a href="engineer-projects-1.html">Data Engineer Projects</a></li>
							<li><a href="elements.html">Elements Reference</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/paul-hankins-075119204/" class="icon brands fa-linkedin"><span class="label">Instagram</span></a></li>
							<li><a href="https://github.com/Piblo99" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">May 04, 2022</span>
									<h1>Serverless Data<br />
                                    Engineering Pipeline: AWS</h1>
								<p>
                                    The objective of this project was create serverless data pipeline
									that would use DynamoDB as a source for data, and push that data into 
                                    AWS SQS using a producer lambda function. Then the data in SQS would be
                                    consumed by a consumer lambda function. The consumer lambda function sits idly
                                    waiting for the producer to run from a cloudwatch event bridge trigger every 1 minutes.
                                    As soon as the producer is pushed to SQS the consumer should take those messages in the Queue
                                    and push them to the AWS Comprehend service. AWS comprehend would perform sentiment analysis
                                    on these messages using wikipedia searches on the names of FANG companies.
                                    After AWS Comprehend generates a sentiment score, the consumer function generats a CSV
                                    file in an Amazon S3 bucket containing the name of the company and the sentiment score
                                    for each message consumed.
                                </p>
                                <div class="image main"><img src="images/ngpipeline.png" alt="" /></div>
								</header>
                                <h2 style="text-align:center;">Setting up AWS Environment</h2>
                                <p>
                                   Firstly, I used AWS IAM to create a role called admin with administrator access permission.
                                   This role would later be used for execution roles for lambda functions.
                                   Then I created a DynamoDB table called fang, which I filled with items that each had one string value called name. 
                                   I created items with name values: Facebook, Amazon (Company), Netflix, and Google. 
                                   Next, I created an SQS named producer and set the visibility timeout window to 2 minutes.
                                   This would be 
                                </p>
                                <h2 style="text-align:center;">Pipeline process</h2>
                                <p>
                                    A picture of the completed pipeline can be seen above, Now I will go into the process of creating this pipeline.
                                    The origin stage is configured to ingest log files from the resources folder within the container running SDC.
                                    The data then goes into a stream selector processor which sends the three different types of logs (Error, Common, and Combined) down different
                                    streams based on string values they contain. Each type of log goes into it's respective
                                    Log parser, which detects the log type and breaks the entire log into multiple name:value fields.
                                    The records then go through a filtering process so we can just get critical errors and logs with the HTTP verb of PATCH.
                                    The apache error logs are generated a bit differently and need some extra processing to generate a proper clientIP field.
                                    Then, all the records move to a GeoIP processor which will use a database of ip addresses and their corresponding cities and other information.
                                    The GeoIP processor is configured to look for the database mmdb file im the resources directory of the SDC container.
                                    After additional information is added to the records, they go through another Stream selector to remove records that have null values in the city field.
                                    I used the field order processor to explicitly order the name:value pairs so they can be properly outputted to a local directory in the SDC container.
                                    Lastly, everything not wanted goes into the trash destination.
                                    <br /><br />
                                    I put the Complete json for the pipeline below in case you want to see the complete configuration in detail and or run the pipeline.
                                    <br /><br />
                                    <a style="color: blue; text-align: center;" href="https://raw.githubusercontent.com/Piblo99/Portfolio/master/peedeepipeline.json">complete pipeline json</a>
                                </p>
                                
                                <h2 style="text-align:center;">Getting the GeoLite2-City database</h2>
                                <p>
                                    Part of this project involves creating a geographical visualization on a dashboard.
                                    To accomplish this, I used the GeoLite2-City database from <a style="color: blue" href="https://www.maxmind.com/">maxmind.com</a>.
                                    Streamsets has a GeoIP processor that can use this database to create Fields containing Longitude
                                    and Latitude values, aswell as the corresponding city names.
                                </p>
                                <h2 style="text-align:center;">Creating the logs</h2>
								<p>
                                    To generate the logs that would be ingested into the pipeline,
                                    I used a tool called <a style="color: blue" href="https://github.com/mingrammer/flog">flog</a>;
                                    A fake log generator for common log formats. 
                                    This tool was extremely helpful and perfectly 
                                    generated the required logs for streamsets, except
                                    for the apache error logs. Streamsets log parsers work
                                    for a different version of the apache error logs than
                                    flog was generating. I needed to edit the log.go file
                                    to generate them in the correct format for SDC. I installed 
                                    flog using <code>go get -u github.com/mingrammer/flog</code>
                                    then I went to the location of the installed files at
                                    C:\Users\[username]\go\pkg\mod\github.com\mingrammer\flog@v0.4.3<br />
                                    I replaced the contents of log.go with the following code:
                                </p>

                                <p>
                                   After I replaced the code, I ran go build and then go install in my terminal.
                                   Now the apache error logs will be generated in a format 
                                   that can be ingested into streamsets without raising an error.
                                   Next I created a folder called Data in my downloads folder.
                                   I opened my terminal and set the current working directory
                                   to this folder and ran the following commands:<br />

                                   <code>flog -t log -f apache_error -o error.log -n 100000 -w</code><br />
                                   <code>flog -t log -f apache_common -o common.log -n 100000 -w</code><br />
                                   <code>flog -t log -f apache_combined -o combined.log -n 100000 -w</code><br />

                                   After the log files are generated I extracted the files of the GeoLite2-City_20220426.tar.gz
                                   into the data folder as well and it was now ready to put into the docker container
                                   where the SDC pipeline is running. To do so, I ran this command on a terminal in my host machine:<br />
                                   <pre><code>docker cp C:\Users\[username]\Downloads\data [container_name]:/resources</code></pre><br />
                                   
                                </p>

                                <h2 style="text-align:center;">Running the Pipeline</h2>
                                <p>
                                    The pipeline runs successfully and starts sending thousands of records every second to
                                    the destination on the local file system. To move this outputted data from the SDC container
                                    to the host machine, we can use docker cp again but in reverse:

                                    <pre><code>docker cp [container_name]:/tmp/out/apacheoutput/_tmp_logdata_0.csv C:\Users[name]\Downloads\data</code></pre>
                                    Now that we have the final result of the pipeline on our host computer, we can put this CSV into tableau to create a visualization.
                                </p>
                                <div class='tableauPlaceholder' id='viz1652020687378' style='position: relative'><noscript><a href='#'><img alt='Dashboard 1 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;lo&#47;logsviz&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='logsviz&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;lo&#47;logsviz&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1652020687378');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';} else { vizElement.style.width='100%';vizElement.style.height='1177px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';vizElement.parentNode.insertBefore(scriptElement, vizElement);</script>
                                <p>
                                    <br/>
                                    This visualization is relatively simple, But I could make something more interesting if the data weren't cleaned so much.
                                    The point of this project was to learn about the capabilites of StreamSets.
                                    It seems like a powerful tool and it is very fun to use!!!
                                </p>
							</section>

					</div>

				<!-- Footer -->
                <footer id="footer">
                    <section class="alt">
                        <h3>Address</h3>
                        <p>821 S JOHNSTONE AVE APT 406<br />
                            BARTLESVILLE, OK 74003-4656</p>
                    </section>
                    <section>
                        <h3>Phone</h3>
                        <p>(504) 201-4748</p>
                    </section>
                    <section>
                        <h3>Email</h3>
                        <p>paulhankins99@gmail.com</p>
                    </section>
                    <section>
                        <h3>Social</h3>
                        <ul class="icons alt">
                            <li><a href="https://www.linkedin.com/in/paul-hankins-075119204/" class="icon brands alt fa-linkedin"><span class="label">Linkedin</span></a></li>
                            <li><a href="https://github.com/Piblo99" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
                        </ul>
                    </section>
            </footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
	</body>
</html>